{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imread : to read an image in a directory \n",
    "#### cvtColor : to convert the colors of an image\n",
    "#### imshow : to show the image \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 1280, 3)\n",
      "(720, 1280)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread('image.jpg')\n",
    "gray =cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "print(np.shape(img))\n",
    "print(np.shape(gray))\n",
    "cv2.imshow(\"colored\",img)\n",
    "cv2.imshow(\"gray\",gray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### VideoCapture(the number of the camera )\n",
    "#### read is video capture version of imread\n",
    "#### imshow is the same \n",
    "#### release() to release  the camera feed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.4) d:\\build\\opencv\\opencv-3.4.4\\modules\\highgui\\src\\window.cpp:364: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9f47c46d9513>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'frame'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m27\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#value of ESC key\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(3.4.4) d:\\build\\opencv\\opencv-3.4.4\\modules\\highgui\\src\\window.cpp:364: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "cap = cv2.VideoCapture (0)#is the no of the camera, import video\n",
    "\n",
    "while True :\n",
    "    _, frame = cap.read()\n",
    "    cv2.imshow('frame',frame)\n",
    "    k=cv2.waitKey(1) \n",
    "    if k == 27: #value of ESC key\n",
    "        break\n",
    "cap.release()#close the camera\n",
    "cv2.destroyAllWindows()#if not the the last pic is still there \n",
    "print(np.shape(frame))#to get the zize of the frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv2.imwrite used to store images in our case frames of video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making your first camera app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os          #change the directory ,save the pics \n",
    "\n",
    "os.mkdir('my_pic')#make dirictory\n",
    "os.chdir('my_pic')#change the dirictory\n",
    "#x='Video Capture'\n",
    "counter=(time.time())#for nameing\n",
    "cap = cv2.VideoCapture (0)\n",
    "\n",
    "while True :\n",
    "    _, frame = cap.read()\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(33) == ord('a'):#once you enter a\n",
    "        cv2.imwrite(\"pic\"+str(counter)+\".jpg\",frame)\n",
    "        counter+=1\n",
    "\n",
    "    k=cv2.waitKey(1) \n",
    "    if k == 27 : #value of ESC key\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(np.shape(frame))#to get the zize of the frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# putting a logo on image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv2.threshold  sits a transition piont \n",
    "cv2.bitwise_not not operation on every pixel\n",
    "cv2.bitwise_and and operation on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img2 = cv2.imread('logo2.png')\n",
    "img1 = cv2.imread('o.jpg')\n",
    "r,c,color =img2.shape #shape return whith the higth , weidth and color \n",
    "\n",
    "logo_Region_In_Img1 = img1[0:r , 0:c]\n",
    "\n",
    "img2gray = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY)# convert it to one dimension\n",
    "ret , mask = cv2.threshold(img2gray,220,255,cv2.THRESH_BINARY_INV)#zero is black\n",
    "#ret has the same value as the threshold\n",
    "\n",
    "mask_inv = cv2.bitwise_not(mask)\n",
    "#same as \n",
    "#ret , mask = cv2.threshold(img2gray,220,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\"\"\"\n",
    "The operation of \"And\" will be performed only if mask[i] doesn't equal zero,\n",
    "else the the result of and operation will be zero. \n",
    "The mask should be either white or black image with single channe\n",
    "\"\"\"\n",
    "\n",
    "img1_bg = cv2.bitwise_and(logo_Region_In_Img1,logo_Region_In_Img1,mask=mask_inv) \n",
    "img2_fg = cv2.bitwise_and(img2,img2,mask=mask) #img2 & img2 if mask!=0\n",
    "\n",
    "dst= cv2.add(img1_bg,img2_fg)\n",
    "img1[0:r , 0:c]= dst\n",
    "#print (mask)\n",
    "#cv2.imshow('img1', img1)\n",
    "cv2.imwrite(\"result.jpg\",img1)\n",
    "#cv2.imshow('m1_bg', img1_bg)\n",
    "#cv2.imshow('img2', img2)\n",
    "#cv2.imshow('m2_fg', img2_fg)\n",
    "#cv2.imshow('mask ', mask)\n",
    "#cv2.imshow('mask_inv', mask_inv)\n",
    "#cv2.imshow('mask_inv2', mask_inv2)\n",
    "#cv2.imshow('gray', img2gray)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# putting a logo on video \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img2 = cv2.imread('logo2.png')\n",
    "r,c,color =img2.shape\n",
    "img2gray = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY)# convert it to one dimension\n",
    "\n",
    "#the mask\n",
    "ret , mask = cv2.threshold(img2gray,220,255,cv2.THRESH_BINARY_INV)\n",
    "#ret has the same value as the threshold\n",
    "#the mask\n",
    "mask_inv = cv2.bitwise_not(mask)\n",
    "#same as \n",
    "#ret , mask = cv2.threshold(img2gray,220,255,cv2.THRESH_BINARY)\n",
    "logo_fg = cv2.bitwise_and(img2,img2,mask=mask)\n",
    "\"\"\"\n",
    "The operation of \"And\" will be performed only if mask[i] doesn't equal zero,\n",
    "else the the result of and operation will be zero. \n",
    "The mask should be either white or black image with single channe\n",
    "\"\"\"\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    _ , frame=cap.read()\n",
    "    logo_Region_In_video = frame[0:r , 0:c]\n",
    "\n",
    "        \n",
    "\n",
    "    logo_bg = cv2.bitwise_and(logo_Region_In_video,logo_Region_In_video,mask=mask_inv)\n",
    "\n",
    "    dst= cv2.add(logo_bg,logo_fg)\n",
    "\n",
    "    frame[0:r , 0:c]= dst\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cv2.imshow('frame',frame)\n",
    "    k=cv2.waitKey(1) \n",
    "    if k == 27 :\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# color detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "cap = cv2.VideoCapture (0)\n",
    "\n",
    "while True :\n",
    "    _, frame = cap.read()\n",
    "    hsv = cv2.cvtColor (frame,cv2.COLOR_RGB2HSV)\n",
    "    #hue, saturation, lightness\n",
    "    \n",
    "    #hsv values of red\n",
    "    lower_red = np.array([130,80,0])\n",
    "    upper_red = np.array ([255,255,255])\n",
    "\n",
    "    mask = cv2.inRange (hsv,lower_red ,upper_red)\n",
    "    \n",
    "    res = cv2.bitwise_and(frame , frame , mask=mask)\n",
    "    \n",
    "    (_,contours,hierarchy)=cv2.findContours(mask,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)#RETR_EXTERNAL  RETR_TREE\n",
    "    \n",
    "    for pic ,contour in enumerate(contours):\n",
    "        area=cv2.contourArea(contour)\n",
    "        if (area>600):\n",
    "            x,y,w,h=cv2.boundingRect(contour)\n",
    "            frame=cv2.rectangle(frame,(x,y),(x+w,y+h),(255,255,255),3)\n",
    "            \n",
    "    #cv2.imshow('res',res)\n",
    "    #cv2.imshow('frame',frame)\n",
    "    #cv2.imshow('mask',mask)\n",
    "    #cv2.imshow('hsv',hsv)\n",
    "    cv2.imshow('frame with color detection',frame)\n",
    "    \n",
    "    k=cv2.waitKey(1) \n",
    "    if k == 27 :\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bluring images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "cap=cv2.VideoCapture(0)\n",
    "while True:\n",
    "    _,frame=cap.read()\n",
    "    blur = cv2.medianBlur(frame,33)\n",
    "    \n",
    "    cv2.imshow(\"blur\",blur)\n",
    "    k=cv2.waitKey(1) \n",
    "    if k == 27 :\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# face detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1bca11231f53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m27\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_cas = cv2.CascadeClassifier('haarcascade_face.xml')\n",
    "\n",
    "cap = cv2.VideoCapture (0)\n",
    "while True  :\n",
    "    _ , frame =cap.read ()\n",
    "    gray =cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cas.detectMultiScale(gray, 1.3, 9)\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "       # cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)#BGR,Thikness of rectangle\n",
    "       cv2.circle(frame,(int(x+w/2),int(y+h/2)),int(w/2),(130,30,0),2)\n",
    "    cv2.imshow('feed',frame)\n",
    "\n",
    "    k=cv2.waitKey(1) \n",
    "    if k == 27 :\n",
    "        break\n",
    "print(type(faces[0]))\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "#sololearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the watch dogs trick =D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_cas = cv2.CascadeClassifier('haarcascade_face.xml')\n",
    "\n",
    "cap = cv2.VideoCapture (0)\n",
    "while True  :\n",
    "    _ , frame =cap.read ()\n",
    "    gray =cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cas.detectMultiScale(gray, 1.3, 5)\n",
    "    for (x,y,w,h) in faces:\n",
    "        \n",
    "        slised=frame[y:y+h,x:x+w]\n",
    "        \n",
    "        blur = cv2.medianBlur(frame,35)\n",
    "        blur[y:y+h,x:x+w]=slised\n",
    "        \n",
    "        \n",
    "\n",
    "    cv2.imshow('feed',blur)\n",
    "\n",
    "    k=cv2.waitKey(1) \n",
    "    if k == 27 :\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_12_10_19_27.jpg\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import cv2\n",
    "imageName = 'DontCare.jpg' #Just a random string\n",
    "cap = cv2.VideoCapture(0)\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #For capture image in monochrome\n",
    "    rgbImage = frame #For capture the image in RGB color space\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Webcam',rgbImage)\n",
    "    #Wait to press 'q' key for capturing\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        #Set the image name to the date it was captured\n",
    "        imageName = str(time.strftime(\"%Y_%m_%d_%H_%M\")) + '.jpg'\n",
    "        #Save the image\n",
    "        cv2.imwrite(imageName, rgbImage)\n",
    "        break\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "#Returns the captured image's name\n",
    "print(imageName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-932b65f68df8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mfaceCascade\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'haarcascade_frontalface_alt2.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Jun  3 15:36:49 2017\n",
    "\n",
    "@author: adam\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "faceCascade = cv2.CascadeClassifier('haarcascade_frontalface_alt2.xml')\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "model = load_model('model_5-49-0.62.hdf5')\n",
    "\n",
    "target = ['angry','disgust','fear','happy','sad','surprise','neutral']\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(gray,scaleFactor=1.1)\n",
    "\n",
    "    # Draw a rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2,5)\n",
    "        face_crop = frame[y:y+h,x:x+w]\n",
    "        face_crop = cv2.resize(face_crop,(48,48))\n",
    "        face_crop = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)\n",
    "        face_crop = face_crop.astype('float32')/255\n",
    "        face_crop = np.asarray(face_crop)\n",
    "        face_crop = face_crop.reshape(1, 1,face_crop.shape[0],face_crop.shape[1])\n",
    "        result = target[np.argmax(model.predict(face_crop))]\n",
    "        cv2.putText(frame,result,(x,y), font, 1, (200,0,0), 3, cv2.LINE_AA)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9f08c2fac4a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m                                       \u001b[1;31m# camera object\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dlib'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "\n",
    "\n",
    "cam = cv2.VideoCapture(0)                                       # camera object\n",
    "f_detector = dlib.get_frontal_face_detector()                   # builtin face detector Deep Learning Model\n",
    "f_predictor = dlib.shape_predictor(\"face_landmarks.dat\")        # features map of the face details, shapes to be detected like nose, eyes\n",
    "\n",
    "while True:\n",
    "    key = cv2.waitKey(1)                                        # wait 1 milliseconds for the user to quit the code if a Q key is pressed\n",
    "    if (key & 0xFF) == ord('q'):#keep just the last 8 bits\n",
    "        break\n",
    "    _, frame = cam.read()                                       # get a numpy 2D 3 channels frame from the camera object\n",
    "    faces = f_detector(frame)                                   # returns list of all faces detected in the frame\n",
    "    for face in faces:                                          # for each face in the detected faces list\n",
    "        landmarks = f_predictor(frame, face)                    # extract the features pixels locations of the detected face\n",
    "        for point in range(68):                                 # the landmarks of the face are combined of 68 points locating the pixels of all the features\n",
    "            x = landmarks.part(point).x                         # x-axis location of the current point\n",
    "            y = landmarks.part(point).y                         # y-axis location of the current point\n",
    "            cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)       # draw a cricle over the given frame the point is placed over the (x, y) of the pixel @ the point of the feauture\n",
    "        pt1 = (face.left(), face.top())                         # the first corner point of the diagonal of the square to be plotted over the detected face\n",
    "        pt2 = (face.right(), face.bottom())                     # the second corner point of the diagonal of the rectangle to be plotted over the detected face\n",
    "        cv2.rectangle(frame, pt1, pt2, color=(0, 255, 0), thickness=2)  # draw the rectangle for the correct corner pixel points\n",
    "    cv2.imshow(\"frame\", frame)                                  # display the frame after edits\n",
    "\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
